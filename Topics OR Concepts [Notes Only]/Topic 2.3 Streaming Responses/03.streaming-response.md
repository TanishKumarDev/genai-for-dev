# **Topic 2.3 — Streaming Responses in Gemini API**

* Followed: 
    * https://ai.google.dev/gemini-api/docs/text-generation#streaming-responses
---

## **1. Concept**

When you normally call:

```js
ai.models.generateContent()
```

You only get the **final response** once Gemini finishes thinking and generating everything.

But in **streaming mode**:

```js
ai.models.generateContentStream()
```

you get partial chunks of the response **as it’s being generated**.

This is extremely useful for:

* Real-time UIs (chat interfaces, dashboards)
* Fast feedback in CLI tools
* Long responses (don’t wait for full generation)

---

## **2. How Streaming Works**

Gemini sends multiple **chunks** of data — each containing a partial text (or JSON fragment).

You can iterate over these chunks using `for await (const chunk of response)`.

Each `chunk` represents a small part of the final output.

---

## **3. Code Example: Streaming Text Output**

**File:** `src/04.streaming-response.js`

```js
// -------------------------------------------------------------
// GOAL: Demonstrate streaming responses using Gemini SDK
// -------------------------------------------------------------
// Concepts:
// - Use generateContentStream() for real-time streaming
// - Process partial text chunks as they arrive
// - Combine chunks to form final output
// -------------------------------------------------------------

import ai from "../utils/geminiClient.js";

// 1️⃣ Define a long prompt for testing
const prompt = `
Explain in detail how artificial intelligence impacts the software development industry.
Include subtopics: productivity, code quality, automation, and future trends.
`;

// 2️⃣ Async main function for streaming
async function main() {
  console.log("Starting stream...\n");

  // Create a stream for incremental output
  const stream = await ai.models.generateContentStream({
    model: "gemini-2.5-flash",
    contents: prompt,
  });

  // 3️⃣ Variable to hold full response
  let fullResponse = "";

  // 4️⃣ Read partial chunks as they arrive
  for await (const chunk of stream) {
    const textPart = chunk.text;
    if (textPart) {
      process.stdout.write(textPart); // prints progressively
      fullResponse += textPart;
    }
  }

  // 5️⃣ Final output once stream ends
  console.log("\n\n--- Final Complete Response ---\n");
  console.log(fullResponse);
}

await main();
```

---

### **Output Example**

```
Starting stream...

Artificial Intelligence (AI) has had a transformative impact on the software development industry...
[more text appears gradually as it’s generated]

--- Final Complete Response ---
Artificial Intelligence (AI) has had a transformative impact on the software development industry...
```

---

## **4. How It Works Internally**

| Step | Description                                                       |
| ---- | ----------------------------------------------------------------- |
| 1    | You call `generateContentStream()` instead of `generateContent()` |
| 2    | Gemini starts sending text chunks as soon as they are ready       |
| 3    | Each chunk is an event containing a partial response              |
| 4    | You can process it immediately (good for UI or CLI)               |
| 5    | The loop ends once the final response is received                 |

---

## **5. Streaming Structured JSON**

You can also stream structured JSON output. Gemini will send **partial JSON fragments**, which you can accumulate and parse at the end.

Example (for structured streaming):

```js
const stream = await ai.models.generateContentStream({
  model: "gemini-2.5-flash",
  contents: "Classify this review: 'The product is fantastic!'",
  config: {
    responseMimeType: "application/json",
    responseJsonSchema: {
      type: "object",
      properties: {
        sentiment: { type: "string", enum: ["positive", "neutral", "negative"] },
        summary: { type: "string" }
      },
      required: ["sentiment", "summary"]
    },
  },
});

let jsonBuffer = "";

for await (const chunk of stream) {
  const text = chunk.candidates?.[0]?.content?.parts?.[0]?.text;
  if (text) {
    jsonBuffer += text;
    process.stdout.write(text);
  }
}

console.log("\nFinal JSON:", JSON.parse(jsonBuffer));
```

---

## **6. Notes for `/Topics OR Concepts/topic2/`**

**File:**
`03.streaming-response.md`

```
# Topic 2.3: Streaming Responses

- generateContentStream() streams partial responses in real time.
- Each chunk is part of the final model output.
- Use `for await (const chunk of stream)` to handle partial data.
- Ideal for chat, dashboards, or CLI interactivity.
- Combine chunks for the final text after stream ends.

Example Use:
- Real-time chat UI
- Progressive JSON data feed
- Large text summarization
```

---

## **7. Practice Exercise**

1. Modify your prompt to be a long essay and observe streaming speed.
2. Add a `Date.now()` timer before and after streaming to measure how much faster streaming feels.
3. Try streaming structured JSON output (sentiment example).

---

## **8. Next Topic Preview**

> **Topic 3: Multi-turn Conversations (Chat Sessions)**

Next you’ll learn:

* How to use `ai.chats.create()`
* Maintain context and history
* Build your own **Chat Agent** that remembers conversation turns.

---
